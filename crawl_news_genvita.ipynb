{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "from csv import DictWriter\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from webbrowser import Chrome\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3548\\3109118352.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome('chromedriver.exe')\n"
     ]
    }
   ],
   "source": [
    "#Declare browser\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "#### ----\n",
    "Source_URL='https://genvita.vn/thu-thach'\n",
    "\n",
    "#Go to the page\n",
    "driver.get(Source_URL)\n",
    "\n",
    "try: #Click the Button to show all pages\n",
    "    while True:\n",
    "        number_of_loops_button=driver.find_element(By.XPATH,\"/html/body/main/section[2]/div/div[3]/a\").click()\n",
    "        sleep(1)\n",
    "except: number_of_loops_button=None\n",
    "sleep(random.randint(1,2))\n",
    "\n",
    "\n",
    "#Get the Title and link\n",
    "elems=driver.find_elements(By.CSS_SELECTOR,\".title [href] \")\n",
    "Title = [elem.text for elem in elems] # list of Titles\n",
    "link=[elem.get_attribute('href') for elem in elems] # List of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for append new row by name of column in csv\n",
    "from csv import writer\n",
    "def append_list_as_row(file_name, list_of_elem):\n",
    "    # Open file in append mode\n",
    "    with open(file_name, 'a+', newline='',encoding='utf-8') as write_obj:\n",
    "        # Create a writer object from csv module\n",
    "        csv_writer = writer(write_obj)\n",
    "        # Add contents of list as last row in the csv file\n",
    "        csv_writer.writerow(list_of_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Platform_Column=\"News\"\n",
    "Source_Name=\"Genvita Thử Thách\"\n",
    "Reaction_total=0\n",
    "Share_total=0\n",
    "Type_Post=\"Post\"\n",
    "Author_Page=\"Page Admin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(0,len(link)):\n",
    "    row_dict_post=[]\n",
    "### Type Post\n",
    "#### ----   \n",
    "    Title_Post=Title[i]\n",
    "    driver.get(link[i])\n",
    "    html_link=BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    \n",
    "    #Section chứa time\n",
    "    info_challenge=html_link.find('section',class_=\"challenge-instruction\") \n",
    "    \n",
    "    try:\n",
    "        # Lấy time thô\n",
    "        info_time=info_challenge.find_all('div',class_=\"content\") \n",
    "        time_clean = re.findall('\\d{1,2}\\/\\d{1,2}\\/\\d{4}',info_time[1].string)[0] #hàm lấy time clean\n",
    "    except: info_challenge=None\n",
    "    \n",
    "    # Get total comments number\n",
    "    comment_total=driver.find_elements(By.CSS_SELECTOR,\"body > main > div > article > section.comment-block.js-comment-block > h3 > span > label\")[0].get_attribute(\"innerHTML\") \n",
    "    # Extract content from header\n",
    "    info_new1=html_link.find('section',class_=\"challenge-detail-header\") \n",
    "    try:\n",
    "        info_new1_clean=info_new1.get_text().replace(\"\\n\\n\",\"\")\n",
    "    except: info_new1=None\n",
    "    # Extract the following content\n",
    "    info_new2=html_link.find('section',class_=\"challenge-detail-content\") \n",
    "    try:\n",
    "        info_new2_clean=info_new2.get_text().replace(\"\\n\\n\",\"\")\n",
    "    except: info_new2=None\n",
    "    # Merge 2 parts from the period ## This is evolve to the extract data\n",
    "    Content_Post= info_new1_clean + info_new2_clean\n",
    "    \n",
    "    # Summary the object to add row in file\n",
    "    row_dict_post=[Platform_Column,Source_Name,Source_URL,link[i],Type_Post,Title_Post,Content_Post,time_clean,Author_Page,comment_total,Reaction_total,Share_total]\n",
    "    append_list_as_row('Output_genvita.csv', row_dict_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Platform_Column=\"News\"\n",
    "Source_Name=\"Genvita Thử Thách\"\n",
    "Author_Comment=[]\n",
    "Type_Comment=\"Comment\"\n",
    "Reaction_total=0\n",
    "Share_total=0\n",
    "Contents_Comment=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---- Comment\n",
    "for a in range(0,len(link)):\n",
    "        driver.get(link[a])\n",
    "        Title_Comment=Title[a]\n",
    "        try: #Button to show more comments \n",
    "                while True:\n",
    "                        click_button_comment=driver.find_element(By.XPATH,\"/html/body/main/div/article/section[6]/div[3]/div[2]/a\").click()\n",
    "                        sleep(1)\n",
    "        except: click_button_comment=None\n",
    "        html_link=BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "        # Get raw comment\n",
    "        comment_text_raw=html_link.find_all( name = \"div\", class_ = 'media-body-top') \n",
    "\n",
    "        for y in range(len(comment_text_raw)-1):\n",
    "                #Declare a list comment for loop\n",
    "                row_dict_comment=[]\n",
    "                # Get clean name\n",
    "                Name_comments = comment_text_raw[y].find( name = \"span\", class_ = 'user-name').get_text()\n",
    "                # Get clean comment\n",
    "                Time_comments=comment_text_raw[y].find( name = \"div\", class_ = 'time').get_text()\n",
    "                # Get clean content\n",
    "                Contents_comment=comment_text_raw[y].find( name = \"p\", class_ = 'comment-content').get_text()\n",
    "                  \n",
    "                # Summary the object to add row in file\n",
    "                row_dict_comment=[Platform_Column,Source_Name,Source_URL,link[a],Type_Comment,Title_Comment,Contents_comment,Time_comments,Name_comments,Reaction_total,Share_total]\n",
    "                # Add data to file\n",
    "                append_list_as_row('Output_genvita.csv', row_dict_comment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9cff5a362bc38ef45d817ae74b1af54d6a076e3d773891282bce078b815ba34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
